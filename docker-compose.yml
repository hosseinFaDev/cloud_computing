services:
  llm-inference-service:
    image: hosseinfadev/llm-inference-image:latest
    ports: 
      - "8080:5000" # Host port 8080 maps to container port 5000
    environment:
      - METRICS_LOG_FILE=/metrics/inside_compose_inference_metrics.csv
    volumes:
      - ./compose_inference_metrics.csv:/metrics/inside_compose_inference_metrics.csv #from host : to container